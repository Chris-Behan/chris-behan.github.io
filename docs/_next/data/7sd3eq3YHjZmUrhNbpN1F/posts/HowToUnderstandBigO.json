{"pageProps":{"postData":{"id":"HowToUnderstandBigO","content":"\nI was recently asked for \"The simplest, quickest, easiest to understand explanation of Big O notation as it relates to programming\". This blog post is my answer.\n\nBig O notation (example: $O(n))$ is a notation used to describe the time it takes code (example: a function) to execute on a given input (example: an array of integers). A fancy term often used to describe the time it takes code to execute is \"time complexity\". If you come across the term \"time complexity\", think \"The speed of the code\".\n\nBig O notation is in the form $O(f(n))$, where $f(n)$ is a *function* of $n$, meaning for every possible value of $n$, $f(n)$ outputs exactly one answer, and that answer does not change.\n\n```jsx\nfunction addOne(n) {\n\treturn n + 1\n}\n```\n\nTo illustrate what I mean by *\"does not change\"*, consider the above function, it does not matter when the function is called, `addOne(n)`will always return n + 1. \n\nAs it relates to programming, we use Big O notation to describe the speed of a piece of code (usually a function). The most common Big O notations, in descending order of speed, are `O(1), O(lg n), O(n), O(n lg n), and O(n^2)`.\n\nI am going to describe each of the common notations with a coding example. The examples are ordered by ease of understanding, not speed. \n\n$O(1)$\n\n```jsx\nfunction constant(n) {\n    return 1\n}\n\nlet a = constant(1234)\nconsole.log(a)\n>>> 1\n```\n\nThe above function has a time complexity (speed) of $O(1)$, which is often referred to as \"constant\". It is referred to as \"constant\" because the time it takes to execute the function is the same (\"constant\") regardless of the size of the input. In the example function above, `constant(n)`will return 1, no matter what the value of n is. \n\nNote: \"c*onstant\"* and 1 are often used interchangeably.\n\n$O(n)$\n\n```jsx\nfunction sumOfList(nums) {\n    let sum = 0;\n    let n = nums.length;\n    for (let i = 0; i < n; i++) {\n        sum += nums[i];\n    }\n    return sum;\n}\n\nlet sum = sumOfList([1, 2, 3])\nconsole.log(sum)\n>>> 6\n```\n\nThe above function takes an array of integers as input and returns its sum. This function has a time complexity of $O(n)$. This is because it loops through every element in `nums`, which has a length of n. \n\nYou may be asking yourself, \"Well what about the time it takes to create the `sum` variable, or the time it takes to run `sum += nums[i]`, shouldn't those be reflected in the Big O notation?\". The answer is that they are. To understand how/why they are included in the notation, we need to introduce another concept called \"Dropping non-dominant terms\". \n\nConsider the function $f(n) = n^2 + n + 1$, this function has a Big O notation of $O(n^2)$. We do not include the n or 1 in the Big O notation because they are insignificant when compared to the dominant term ($n^2$) . \n\nExample:\n\n$$\nf(n) = n^2 + n + 1\n$$\n$$\nf(100) = 100^2 + 100 +1 \n$$\n$$\nf(100) = 10000 + 100 + 1 \n$$\n\nNotice how the non-dominant terms, $n$ and 1, are insignificant compared to the dominant term $n^2$, and as $n$ gets larger, they only become more insignificant. In Big O notation, we ignore non-dominant terms completely and just say that $f(n) = O(n^2)$\n\nNow back to our original example, lets break down the time it takes to perform each line, assigning a value of 1 to lines that take \"constant\" time:\n\n```jsx\nfunction sumOfList(nums) {\n    let sum = 0; // 1\n    let n = nums.length; // 1\n    for (let i = 0; i < n; i++) { // everything in this for loop is run n times\n        sum += nums[i]; // 1\n    }\n    return sum; // 1\n}\n```\n\nIf we add up the time it takes each line to execute, and account for the for loop, we get:\n\n$$\n1 + 1 + (n * 1) + 1\n$$\n$$\n= 1 + 1 + n + 1\n$$\n$$\n= 3 + n\n$$\n\nWe then drop the non-dominant terms and get a Big O notation of $O(n)$.\n\n$O(n^2)$\n\n```jsx\nfunction generateAllPairs(numbers) {\n  const pairs = [];\n  const n = numbers.length;\n  for (let i = 0; i < n; i++) {\n    for (let j = 0; j < n; j++) {\n      pairs.push([numbers[i], numbers[j]]);\n    }\n  }\n  return pairs;\n}\n```\n\nThe above function generates all possible pairs that can be created from a list of numbers. For example, `generateAllPairs([1,2,3]) = [[1,1], [1,2], [1,3], [2,1], [2,2], [2,3], [3,1], [3,2],[3,3]]`. This function has a time complexity of $O(n^2)$. This is because, for every element in `numbers` (which has a length of $n$), we do some processing on $n$ other elements. \n\nLet's break down the function line by line:\n\n```jsx\nfunction generateAllPairs(numbers) {\n  const pairs = []; // 1\n  const n = numbers.length; // 1\n  for (let i = 0; i < n; i++) { // everything in this for loop runs n times\n    for (let j = 0; j < n; j++) { // everything in this for loop runs n times\n      pairs.push([numbers[i], numbers[j]]); // 1\n    }\n  }\n  return pairs; // 1\n}\n```\n\nThe important part of the above code is the *nested* for loop (where nested means inside another for loop). The nested for loop \n\n```jsx\nfor (let j = 0; j < n; j++) {\n```\n\nexecutes \n```jsx\npairs.push([numbers[i], numbers[j]]); // takes constant time\n``` \n$n$ times, giving it a time complexity of $n * 1 = n$. \n\nHowever, this for loop is within another for-loop \n```jsx\nfor (let i = 0; i < n; i++)\n```\nwhich  executes all code inside itself $n$ times.\n\nAdding up the cost of every line and dropping the non-dominant terms, we get:\n\n$$\n1 + 1 + (n *(n*1))+1\n$$\n$$\n= 3 + (n*(n*1))\n$$\n$$\n= 3 + (n*n)\n$$\n$$\n= 3 + n^2\n$$\n$$\n= n^2\n$$\n\nThus the time complexity of `generateAllPairs` is $O(n^2)$.\n\nAs a rule of thumb, if you see a nested for loop,\n\n```jsx\nfor (let i = 0; i < n; i++) {\n\tfor (let j = 0; j < n; j++) {\n\t\t//do stuff\n\t}\n}\n```\n\nwhere both loops have the same conditional statement (the conditional statement of a for loop is the statement that determines whether or not to perform an iteration, in the above loops, the conditional statements are `i < n` and `j < n` ) and the same increment (the increments in the above loops are `i++` and `j++` ), then think $O(n^2)$.\n\n$O(lg\\ n)$\n\n```jsx\nfunction base2Log(num) {\n  let count = 0;\n  for (; num > 1; num /= 2) {\n    count++;\n  }\n  if (num == 1) {\n    return count;\n  }\n  throw new Error(`You must input a multiple of 2!`);\n}\n```\n\nThe above function returns the base 2 logarithm for multiples of 2. In other words, It is a function that returns the answer to $\\log_2{num}$, where $num$ is a multiple of 2. It has a time complexity of $O(lg\\ n)$. Unlike previous examples which took an array of numbers as input, `num` in this function is a number, which we will treat as our $n$.\n\n\"Well how do you know what the $n$ is, if sometimes it can be an array of numbers, and other times a number itself?\" Good question, the $n$ of a piece of code as it relates to Big O notation is the input (usually a parameter) to a function that effects how many iterations the code performs. The type of the input does not matter, but rather the size of the input, and how that size effects the number of iterations performed.\n\nExample 1:\n\n```jsx\nfunction exampleOne(amount) {\n\tfor (let i = 0; i < amount; i++) {\n\t\t// do something\n\t}\n}\n```\n\nHere $n$ would be the `amount` parameter (a number), since the value of `amount` effects how many iterations are performed by the for loop. \n\nExample 2:\n\n```jsx\nfunction exampleTwo(amount) {\n\tif (amount == 1) {\n\t\treturn 1;\n\t}\n\treturn 1 + exampleTwo(amount - 1)\n}\n```\n\nThe $n$ in this example is also `amount`, however instead of using a loop like other examples, we use recursion. The time complexity of this example is $O(n)$ since we make $n-1$ recursive calls, and each recursive call can be thought of as an iteration.\n\nExample 3:\n\n```jsx\nfunction exampleThree(amount) {\n\tconst n = amount.length;\n\tfor (let i = 0; i < n; i++) {\n\t\t// do something\n\t}\n}\n```\n\nHere `amount` is an array of numbers. The $n$ in this example is the *length* of `amount`, since the length determines how many iterations are performed by the for loop.\n\nNow back to our $O(lg\\ n)$ example, lets break it down line by line:\n\n```jsx\nfunction base2Log(num) {\n  let count = 0; // 1\n  for (; num > 1; num /= 2) { // everything in this for loop runs lg num times\n    count++; // 1\n  }\n  if (num == 1) { // 1\n    return log; // 1\n  }\n  throw new Error(`You must input a multiple of 2!`);\n}\n```\n\nThe for loop in the above code divides `num` by 2 each iteration, until `num` â‰¤ 1. For values of `num` that are multiples of 2, this loop will perform exactly $lg\\ n$ iterations, where n = `num`. \n\nNote: the shorthand for $log_2$ is $lg$. \n\nAdding the cost of each line and dropping non-dominant terms we get:\n\n$$\n1 + (lg\\ n*  1) + 1 + 1\n$$\n$$\n= 3 + lg\\ n\n$$\n$$\n= lg\\ n\n$$\n\nThus the time complexity of `base2Log` is $O(lg\\ n)$.\n\nTo understand why this loop performs exactly `lg n` iterations when `num` is a multiple of 2, you have to understand what a logarithm tells us. $\\log_2{num}$ is the equivalent of saying \"2 to the power of what equals $num$.\" So for example: $\\log_2{8}$ is the equivalent of asking \"2 to the power of what equals 8?\". The answer is 3, since $2 * 2 * 2 = 2^3 = 8$. This can be rewritten as $1 * 2 * 2 * 2 = 8$. What the above function does is count the number of times `num` needs to be divided by 2 until it equals 1, which is the equivalent of answering $\\log_2{num}$.\n\nExample:\n\n$\n\\textnormal{before 1st iteration, } num = 8,\\ count = 0\n$\n$\n\\textnormal{after 1st iteration, } num = 8/2 = 4,\\ count = 1\n$\n$\n\\textnormal{after 2nd iteration, }num = 4/2=2,\\ count = 2\n$\n$\n\\textnormal{after 3rd iteration, }num = 2/2=1,\\ count = 3\n$\n\n$\n\\textnormal{return}\\ count = 3\n$\n\nAs a rule of thumb, if you see a for loop whose loop counter is divided by a number each iteration,\n```jsx\nfor(let i = 100; i > 1; i = i / 2) {\n  // do stuff\n}\n```\nthink $O(lg\\ n)$.\n\n$O(n\\ lg\\ m)$\n\n```jsx\nfunction base2LogList(nums) {\n\tconst base2Logs = []\n\tfor (let i = 0; i < nums.length; i++) {\n\t\tconst log = base2Log(nums[i]);\n\t\tbase2Logs.push(log); \n\t}\n\treturn base2Logs;\n}\n\nfunction base2Log(num) {\n  let count = 0;\n  for (; num > 1; num /= 2) {\n    count++;\n  }\n  if (num == 1) {\n    return log;\n  }\n  throw new Error(`You must input a multiple of 2!`);\n}\n```\n\nThe `base2LogList` function returns the base 2 logarithms for a list of numbers. For example, `base2LogList([2,4,8,16])` would return `[1, 2, 3, 4]`. The time complexity of this function is $O(n\\ lg\\ m)$, where $n$ is the *length* of `nums` and where $m$ is the largest number in `nums`. \n\nThis example introduces two new concepts:\n\n1. Calling a function within another function\n2. Different variables within the Big O\n\n**Calling a function within another function**\n\nSuppose we have a function called `innerFunction` with a time complexity of $O(n)$.\n\n```jsx\nfunction innerFunction(n) {\n\tfor (let i = 0; i < n; i++) {\n\t\t// do stuff\n\t}\n}\n```\n\nNow suppose we have another function, `outerFunction`, which calls `innerFunction`, $n$ times.\n\n```jsx\nfunction outerFunction(n) {\n\tfor (let i = 0; i < n; i++) { // everything in the for loop runs n times\n\t\tinnerFunction(n) // n\n\t}\n}\n```\n\nWhen evaluating the time complexity of `outerFunction`, we treat the cost of the line that calls `innerFunction` as $n$, since `innerFunction` has a time complexity of $O(n)$.\n\nSince `outerFunction` calls `innerFunction` $n$ times, and the cost of calling `innerFunction` is $n$, `outerFunction` has a time complexity of $O(n^2)$.\n\n**Different variables within the Big O**\n\nIf there are multiple inputs that effect the number of iterations performed by a function, we use a different variable to represent each input.\n\nExample:\n\n```jsx\nfunction loopSum(n, m) { \n\tlet sum = 0; // 1\n\tfor (let i = 0; i < n; i++) { // everything in this for loop runs n times\n\t\tsum++; // 1\n\t}\n\n\tfor (let j = 0; j < m; j++) { // everything in this for loop runs m times\n\t\tsum++; // 1\n\t}\n\treturn sum; // 1\n}\n```\n\nThe above function has a time complexity of $O(n + m)$. This is because it performs $m$ iterations, than $n$ iterations, and $m$ and $n$ are separate/distinct values.\n\n$$\n1 + (n * 1) + (m * 1) + 1\n$$\n$$\n= 2 + (n*1) + (m * 1)\n$$\n$$\n=2 + n + m\n$$\n$$\n= n + m\n$$\n\nBack to our $O(n\\ lg\\ m)$ example, breaking it down line by line:\n\n```jsx\nfunction base2LogList(nums) {\n\tconst base2Logs = [] // 1\n\tfor (let i = 0; i < nums.length; i++) { // everything in this loop runs nums.length times\n\t\tconst log = base2Log(nums[i]); // lg nums[i]\n\t\tbase2Logs.push(log); // 1\n\t}\n\treturn base2Logs; // 1\n}\n```\n\nThe for loop above performs `nums.length` iterations. Within each iteration, a call to `base2Log` using the input parameter `nums[i]` is made. Since `base2Log` has a time complexity of $O(lg\\ n)$, and we are passing it an input of `nums[i]`, the cost of this line is `lg nums[i]`. \n\nLetting $n$ = `nums.length`, and $m$ = `nums[i]`, and adding the cost of each line, we get:\n\n$$\n1 + (n*(lg\\ m\\ +1)) + 1\n$$\n$$\n= 2+ (n*(lg\\ m + 1))\n$$\n$$\n= 2 + n*lg\\ m + n\n$$\n$$\n=n* lg\\ m \n$$\n\nThus, the time complexity of `base2LogList` is $O(n\\ lg\\ m)$. \n\nIf you confused how we went from:\n\n$$\n2 + n * lg\\ m + n \n$$\n$$\n\\textnormal{to:} \n$$\n$$\nn * lg\\ m\n$$\n\nRemember the rule of \"Dropping-non dominant terms\", for large values of $n$ and $m$, $n$ and 2 are insignificant compared to the dominant term $n* lg\\ m$.\n\n# Conclusion\nLooking back at the original inquiry for \"The simplest, quickest, easiest to understand explanation of Big O notation as it relates to programming\", I am not sure my answer is simple, quick, or easy to understand. I hope at the very least that some of my practical examples and explanations have improved your understanding of Big O notation and made you slightly more equipped to evaluate the speed of code. \n\n# Further Reading\n\nIf you are interested in learning more about Big O notation (also referred to as *asymptotic notation)* I recommend the \"Big O\" section of *[Cracking the Coding Interview](https://www.amazon.ca/Cracking-Coding-Interview-Programming-Questions/dp/0984782850/ref=sr_1_1?crid=122ZRWCQGEKK&dchild=1&keywords=cracking+the+coding+interview&qid=1607900235&sprefix=cracking%2Caps%2C241&sr=8-1)* (practical explanation) and the chapter on \"Growth of Functions\" in *[Introduction to Algorithms](https://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=sr_1_2?dchild=1&keywords=algorithms&qid=1607900650&sr=8-2)* (theoretical explanation).","title":"How to Understand Big O","description":"A practical explanation of Big O with code examples.","date":"2020-12-13"}},"__N_SSG":true}